{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L7RK4PKFnkq"
      },
      "source": [
        "# Energy-based PPO\n",
        "#### Team MIRAM\n",
        "---\n",
        "\n",
        "Makar Korchagin, Ilya Zherebtsov, Rinat Prochii, Aibek Akhmetkazy, Mikhail Gubanov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A5EvCu2FzD7"
      },
      "source": [
        "For the third project we have chosen the Inverted Double Pendulum environment with the custon reward function. The aim of the enviornmnet is to balance the Inverted Double Pendulum via controlling the movements of the cart, to which the pendulum is attached.\n",
        "\n",
        "![Inverted double pendulum](https://gymnasium.farama.org/_images/inverted_double_pendulum.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfaf2-N-GXjq"
      },
      "source": [
        "## State space\n",
        "\n",
        "![Double pendulum system](https://www.researchgate.net/profile/Slavka-Jadlovska-2/publication/258795979/figure/fig2/AS:392622284787714@1470619855067/Classical-double-inverted-pendulum-system-scheme-and-basic-nomenclature.png)\n",
        "\n",
        "The system has the state space of 12 continuous variables:\n",
        "\n",
        "$x \\in \\mathbb{R}$ - is the position of the cart with mass $m$;\n",
        "\n",
        "$\\dot{x} \\in \\mathbb{R}$ - is the speed of the cart with mass $m$;\n",
        "\n",
        "$\\ddot{x} \\in \\mathbb{R}$ - is the acceleration of the cart with mass $m$;\n",
        "\n",
        "$\\theta_1 \\in [0, 2\\pi]$ - is the angle of the hinge with mass $m_1$ w.r.t. the vertical axis;\n",
        "\n",
        "$\\theta_2 \\in [0, 2\\pi]$ - is the angle of the hinge with mass $m_2$ w.r.t. the vertical axis;\n",
        "\n",
        "$\\dot{\\theta}_1 \\in \\mathbb{R}$ - is the anglular velocity of the hinge with mass $m_1$;\n",
        "\n",
        "$\\dot{\\theta}_2 \\in \\mathbb{R}$ - is the anglular velocity of the hinge with mass $m_2$;\n",
        "\n",
        "$\\ddot{\\theta}_1 \\in \\mathbb{R}$ - is the anglular acceleration of the hinge with mass $m_1$;\n",
        "\n",
        "$\\ddot{\\theta}_2 \\in \\mathbb{R}$ - is the anglular acceleration of the hinge with mass $m_2$;\n",
        "\n",
        "$f_0, f_1, f_2 \\in \\mathbb{R}$ - the friction forces for each degree of freedom.\n",
        "\n",
        "Aside from the state vector, the system has 5 hyperparameters:\n",
        "\n",
        "$m$ - the mass of the cart;\n",
        "\n",
        "$m_1, m_2$ - the masses of the first and second hinges respectively;\n",
        "\n",
        "$J_1, J_2$ - inertia momentums of the first and second poles respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4J-8R_hERu"
      },
      "source": [
        "### Mathematical model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz9dnQHrhPqo"
      },
      "source": [
        "Using the state variables and hyperparameters defined above, we can derive the second-order differential equation, that describes the behaviour of the given system\n",
        "\n",
        "$$H_1(z)\\ddot{z}=H_2(z,\\dot{z})\\dot{z}+h_3(z)+h_0u,$$\n",
        "\n",
        "where\n",
        "\n",
        "$$z = (x, \\theta_1, \\theta_2)^T;$$\n",
        "\n",
        "$$H_1(z) =\n",
        "\\begin{bmatrix}\n",
        "a_0 & a_1\\cos\\theta_1 & a_2\\cos\\theta_2 \\\\\n",
        "a_1\\cos\\theta_1 & b_1 & a_2l_1\\cos(\\theta_2-\\theta_1) \\\\\n",
        "a_2\\cos\\theta_2 & a_2l_1\\cos(\\theta_2 - \\theta_1) & b_2 \\\\\n",
        "\\end{bmatrix};\n",
        "$$\n",
        "\n",
        "$$H_2(z, \\dot{z}) =\n",
        "\\begin{bmatrix}\n",
        "-f_0 & a_1\\sin\\theta_1\\dot{\\theta}_1 & a_2\\sin\\theta_2\\dot{\\theta}_2 \\\\\n",
        "0 & - f_1 - f_2 & a_2l_1\\sin\\theta_2 \\dot{\\theta}_2 \\\\\n",
        "0 & -a_2l_1\\sin(\\theta_2 - \\theta_1)\\dot{\\theta}_1 + f_2 & - f_2 - f_3 \\\\\n",
        "\\end{bmatrix};\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_3(z) =\n",
        "\\begin{bmatrix}\n",
        "0 & a_1g\\sin\\theta_1 & a_2g\\sin\\theta_2\n",
        "\\end{bmatrix}^T.\n",
        "$$\n",
        "\n",
        "The constants above denotes the following formulae\n",
        "\n",
        "$$a_0 = m + m_1 + m_2;$$\n",
        "\n",
        "$$a_1 = m_1l_1 + m_2l_2;$$\n",
        "\n",
        "$$a_2 = m_2l_2;$$\n",
        "\n",
        "$$b_1 = J_1 + m_1l_1^2 + m_2l_2^2;$$\n",
        "\n",
        "$$b_2 = J_2 + m_2l_2^2.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIdm5GT7igLT"
      },
      "source": [
        "### Initial state\n",
        "\n",
        "The initial positions of the poles and cart are randomly sampled from uniformly distributed noise, while the initial velocities are randomly sampled from the normal distributions. The second derivatives and other variables are zeros at the initial state\n",
        "\n",
        "$$s_0 =\n",
        "\\begin{cases}\n",
        "(\\theta_1, \\theta_2, x) \\sim \\mathcal{U}(-0.1 \\times I_3, 0.1 \\times I_3); \\\\\n",
        "(\\dot{\\theta}_1, \\dot{\\theta}_2, \\dot{x}) \\sim \\mathcal{N}(0_3, 0.1 \\times I_3); \\\\\n",
        "(\\ddot{\\theta}_1, \\ddot{\\theta}_2, \\ddot{x}, f_0, f_1, f_2) = 0_6.\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWOseHjZhRv1"
      },
      "source": [
        "### State space outline\n",
        "\n",
        "In conclusion, the environment of the Inverted Double Pendulum has a state vector of $s \\in \\mathbb{S} \\subset \\mathbb{R}^{12}$ with 9 variables of unlimited real numbers and two variables limited by the interval of $[0, 2\\pi]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBJFTsjiK02_"
      },
      "source": [
        "## Action space\n",
        "\n",
        "The action space is a single continuous variable $f \\in [-1, 1]$ denoting the force [N] applied to the moving cart along $X$ axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyJkheFiLR8X"
      },
      "source": [
        "## Observation space\n",
        "\n",
        "The environment support two types of observation spaces: kinmatics vector and the RGB image of the system. In this project we have decided to choose the first option.\n",
        "\n",
        "The observation vector of kinematics type includes 11 continuous variables:\n",
        "\n",
        "$x \\in \\mathbb{R}$ - is the position of the cart along x axis;\n",
        "\n",
        "$\\sin\\theta_1 \\in [-1, 1]$ - is the sine of the angle of the first hinge;\n",
        "\n",
        "$\\sin\\theta_2 \\in [-1, 1]$ - is the sine of the angle of the second hinge;\n",
        "\n",
        "$\\cos\\theta_1 \\in [-1, 1]$ - is the cosine of the angle of the first hinge;\n",
        "\n",
        "$\\cos\\theta_2 \\in [-1, 1]$ - is the cosine of the angle of the second hinge;\n",
        "\n",
        "$u \\in \\mathbb{R}$ - is the velocity of the cart;\n",
        "\n",
        "$\\dot{\\theta}_1 \\in \\mathbb{R}$ - is the angular velocity of the first hinge;\n",
        "\n",
        "$\\dot{\\theta}_2 \\in \\mathbb{R}$ - is the angular velocity of the first hinge;\n",
        "\n",
        "$f_1, f_2, f_3 \\in \\mathbb{R}$ - are the contraints for each degree of freedom (cart pole position, first and second hinge angles respectively).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2RP0y68PSsb"
      },
      "source": [
        "## Reward\n",
        "\n",
        "The original reward introduce the constant reward for every step that can be decreased because of low position of the pendulum's tip and high speed of the cart\n",
        "\n",
        "$$r(s,a) = 10 - (0.01 x^2 + (y-2)^2) - 0.001 v_1^2 + 0.005 v_2^2,$$\n",
        "\n",
        "where:\n",
        "\n",
        "$x, y$ - are the coordinates of the free tip of the pendulum;\n",
        "\n",
        "$v_1, v_2$ - are the absolute velocities of the poles' centres of masses.\n",
        "\n",
        "We decided to change the reward, using the physics assumptions. It is obvoius that the pendulum holding is the task of the potential energy $V$ maximization and kinetic energy $T$ minimization\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "V \\rightarrow \\max{}; \\\\\n",
        "T \\rightarrow 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Essentially, we propose is to equalize the reward function to the negative Lagrangian of the system\n",
        "\n",
        "$$r_E(s,a) = -L = \\sum_{i=0}^2 V_i - T_i,$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$T_0 = \\frac{m\\dot{x}^2}{2}, V_0 = 0;$$\n",
        "\n",
        "$$T_1 = \\frac{m_1v_1^2}{2} + \\frac{J_1 \\dot{\\theta}_1^2}{2}=\\frac{7}{24}m_1l_1^2\\dot{\\theta}_1^2, V_1 = \\frac{1}{2}m_1gl_1\\cos\\theta_1;$$\n",
        "\n",
        "$$T_2 = \\frac{m_2v_2^2}{2} + \\frac{J_2 \\dot{\\theta}_2^2}{2}= \\frac{m_2l_2^2}{2} \\left( \\dot{\\theta}_1^2 + \\frac{1}{4} \\dot{\\theta}_2^2 + \\dot{\\theta}_1\\dot{\\theta}_2\\cos(\\theta_1 - \\theta_2)\\right) + \\frac{m_2l_2\\dot{\\theta}_2^2}{6}, V_2 = m_2g\\left(l_1\\cos\\theta_1 + \\frac{1}{2}l_2\\cos\\theta_2 \\right).$$\n",
        "\n",
        "Moreover, the proposed reward can be calculated using the observation vector and hyperparameters values only.\n",
        "\n",
        "For simplicity, the reward function can be normalized to the masses $m_1 = m_2 = m = 1$ and lengths $l_1 = l_2 = 1$ of the poles\n",
        "\n",
        "$$\n",
        "r_E(s,a) = g\\cos\\theta_1 + \\frac{1}{2}g\\cos\\theta_2 - \\frac{19}{24}\\dot{\\theta}_1^2 - \\frac{7}{24}\\dot{\\theta}_2^2 - \\frac{1}{2}\\dot{\\theta}_1\\dot{\\theta}_2\\cos(\\theta_1 - \\theta_2).\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reward normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evidently, the rewards have different ranges. To make them comparable, we decided to normalize both of them to their maximum values.\n",
        "\n",
        "For original reward the maximum value is\n",
        "\n",
        "$$\\max{r(s,a)} = 10, \\text{if}\\, x, y, v_1, v_2 = 0.$$\n",
        "\n",
        "For energy-based reward\n",
        "\n",
        "$$\\max{r_E(s,a)} = \\frac{1}{2}g (2m_2l_1 + m_2l_2 + m_1l_1), \\text{if}\\, \\theta_1, \\theta_2, \\dot{x}, \\dot{\\theta_1}, \\dot{\\theta_2} = 0.$$\n",
        "\n",
        "For $l_1 = l_2 = 0.6$ and $m_1 = m_2 = 1$ the maximum reward is\n",
        "\n",
        "$$\\max{r_E} = \\frac{1}{2} \\cdot 9.81 (2\\cdot0.6 + 0.6 + 0.6) = 9.81 \\cdot 1.2 = 11.772.$$\n",
        "\n",
        "Thus, normalizing both rewards to their maximum values, we may compare the performance of both agents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reward implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reward has been implemented as a class with `__call__` method for reward computation.\n",
        "\n",
        "```python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "ELEMENTS = 3\n",
        "\n",
        "class EnergyReward:\n",
        "    def __init__(\n",
        "        self,\n",
        "        pole1_length: float = 0.6,\n",
        "        pole2_length: float = 0.6,\n",
        "        cart_mass: float = 1,\n",
        "        pole1_mass: float = 1,\n",
        "        pole2_mass: float = 1,\n",
        "        g: float = 9.81,\n",
        "    ):\n",
        "        \"\"\"Init environment hyperparameters.\"\"\"\n",
        "        self._cart_mass = cart_mass\n",
        "        self._mass1 = pole1_mass\n",
        "        self._mass2 = pole2_mass\n",
        "        self._length1 = pole1_length\n",
        "        self._length2 = pole2_length\n",
        "        self._g = g\n",
        "\n",
        "    def __call__(self, observation: np.ndarray) -> float:\n",
        "        \"\"\"Calculate the energy-based reward.\"\"\"\n",
        "        sin1, sin2, cos1, cos2, dx, dtheta1, dtheta2 = observation[1:-1]\n",
        "\n",
        "        e_t = np.zeros(shape=ELEMENTS)\n",
        "        e_v = np.zeros(shape=ELEMENTS)\n",
        "\n",
        "        # Calculate potential energies of the cart, first and second poles\n",
        "        e_v[0] = 0\n",
        "        e_v[1] = self._mass1 * self._g * self._length1 / 2 * cos1\n",
        "        e_v[2] = self._mass2 * self._g * (\n",
        "            self._length2 / 2 * cos2 + self._length1 * cos1\n",
        "            )\n",
        "\n",
        "        cos_diff = cos1 * cos2 + sin1 * sin2\n",
        "        # Calculate the kinetic energies of the corresponding elements\n",
        "        e_t[0] = self._cart_mass / 2 * dx**2\n",
        "        e_t[1] = 7 / 24 * self._mass1 * self._length1**2 * dtheta1**2 \n",
        "        e_t[2] = self._mass2 * self._length2 * dtheta2**2 / 6\n",
        "        e_t[2] += self._mass2 * self._length2**2 / 2 * (\n",
        "            dtheta1**2 + dtheta2**2 / 4 + dtheta1 * dtheta2 * cos_diff\n",
        "            )\n",
        "\n",
        "        return np.sum(e_v - e_t)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from src.trpo import TRPO\n",
        "from src.tools import train, evaluate, load_model\n",
        "from src.utils import mp4_to_gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Set environment name\n",
        "ENV_NAME = \"InvertedDoublePendulum-v5\"\n",
        "os.makedirs(\"results\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device (GPU if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train\n",
        "We now train the agent using the configuration we define. The training loop will print progress, and after training we obtain training statistics such as average episode rewards and episode lengths. \n",
        "\n",
        "In train config we can choose the `reward_type` we need (`\"rewards\"` for the standart or `\"energies\"` for energy based) and the `agent` we need (`TRPO` or `PPO`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exmaple for energy based reward for TRPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TRPO training: 100%|██████████| 10/10 [00:32<00:00,  3.27s/it]\n"
          ]
        }
      ],
      "source": [
        "agent = train(\n",
        "    env_name=ENV_NAME,\n",
        "    agent=TRPO,\n",
        "    num_epochs=10,\n",
        "    steps_per_epoch=4096,\n",
        "    gamma=0.99,\n",
        "    reward_type=\"energies\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training code snippet**\n",
        "\n",
        "```python\n",
        "def train(\n",
        "    env_name: str,\n",
        "    agent: PPO | TRPO,\n",
        "    num_epochs: int = 500,\n",
        "    steps_per_epoch: int = 4096,\n",
        "    gamma: float = 0.99,\n",
        "    reward_type: str = \"reward\",\n",
        "    seed: int | None = None,\n",
        "):\n",
        "    # Create environment\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Get dimensions\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    # Create agent\n",
        "    agent = agent(state_dim, action_dim, gamma=gamma)\n",
        "    name = agent.__class__.__name__.lower()\n",
        "\n",
        "    # Create energy reward calculator\n",
        "    energy_reward_func = EnergyReward()\n",
        "    best_reward = -np.inf\n",
        "\n",
        "    # Create CSV logger\n",
        "    os.makedirs(\"./results\", exist_ok=True)\n",
        "    csv_file = open(f\"./results/{name}-train-{reward_type}.csv\", \"w\", newline=\"\")\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\n",
        "        \"epoch\",\n",
        "        \"length\",\n",
        "        \"reward\",\n",
        "        \"energy\",\n",
        "        \"policy_loss\",\n",
        "        \"value_loss\",\n",
        "        \"kl\",\n",
        "        \"entropy\",\n",
        "    ])\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm.trange(num_epochs, desc=f\"{name.upper()} training\"):\n",
        "        # Collect trajectories\n",
        "        trajectories = collect_trajectories(\n",
        "            env, agent, energy_reward_func, steps_per_epoch, seed\n",
        "        )\n",
        "\n",
        "        # Update agent\n",
        "        update_info = agent.update(\n",
        "            trajectories[\"states\"],\n",
        "            trajectories[\"actions\"],\n",
        "            trajectories[reward_type],\n",
        "            trajectories[\"masks\"],\n",
        "        )\n",
        "\n",
        "        # Write to CSV\n",
        "        for i in range(len(trajectories[\"episode_lengths\"])):\n",
        "            csv_writer.writerow([\n",
        "                epoch,\n",
        "                trajectories[\"episode_lengths\"][i],\n",
        "                trajectories[\"episode_rewards\"][i],\n",
        "                trajectories[\"episode_energies\"][i],\n",
        "                update_info[\"policy_loss\"],\n",
        "                update_info[\"value_loss\"],\n",
        "                update_info[\"kl\"],\n",
        "                update_info[\"entropy\"],\n",
        "            ])\n",
        "        csv_file.flush()\n",
        "\n",
        "        # Save model periodically\n",
        "        if np.mean(trajectories[reward_type]) > best_reward:\n",
        "            best_reward = np.mean(trajectories[reward_type])\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"policy\": agent.policy.state_dict(),\n",
        "                    \"value\": agent.value.state_dict(),\n",
        "                },\n",
        "                f\"./results/{name}-{reward_type}-best.pt\",\n",
        "            )\n",
        "\n",
        "    csv_file.close()\n",
        "    env.close()\n",
        "    return agent\n",
        "```\n",
        "\n",
        "**Agent code snippet**\n",
        "\n",
        "Policy snippet\n",
        "\n",
        "```python\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 64):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.mean = nn.Linear(hidden_dim, output_dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mean = self.mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def get_action(self, state, deterministic=False):\n",
        "        mean, std = self.forward(state)\n",
        "        if deterministic:\n",
        "            return mean\n",
        "        else:\n",
        "            dist = Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            return action\n",
        "\n",
        "    def log_prob(self, state, action):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = Normal(mean, std)\n",
        "        return dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "    def get_kl(self, state, other):\n",
        "        mean1, std1 = self.forward(state)\n",
        "        mean2, std2 = other.forward(state)\n",
        "\n",
        "        dist1 = Normal(mean1, std1)\n",
        "        dist2 = Normal(mean2, std2)\n",
        "\n",
        "        kl = torch.distributions.kl.kl_divergence(dist1, dist2).sum(dim=-1).mean()\n",
        "        return kl\n",
        "```\n",
        "\n",
        "Value code snippet\n",
        "\n",
        "```python\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 64):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x).squeeze()\n",
        "```\n",
        "\n",
        "TRPO code snippet\n",
        "\n",
        "```python\n",
        "class TRPO:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 64,\n",
        "        gamma: float = 0.99,\n",
        "        tau: float = 0.95,\n",
        "        delta: float = 0.01,\n",
        "        damping: float = 0.1,\n",
        "        cg_iters: int = 10,\n",
        "        backtrack_iters: int = 10,\n",
        "        backtrack_coeff: float = 0.8,\n",
        "    ):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.delta = delta\n",
        "        self.damping = damping\n",
        "        self.cg_iters = cg_iters\n",
        "        self.backtrack_iters = backtrack_iters\n",
        "        self.backtrack_coeff = backtrack_coeff\n",
        "\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        self.value = ValueNetwork(state_dim, hidden_dim)\n",
        "        self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=5e-4)\n",
        "\n",
        "    def _compute_advantages(self, rewards, values, masks):\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        gae = 0\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value = 0\n",
        "            else:\n",
        "                next_value = values[t + 1]\n",
        "\n",
        "            delta = rewards[t] + self.gamma * next_value * masks[t] - values[t]\n",
        "            gae = delta + self.gamma * self.tau * masks[t] * gae\n",
        "\n",
        "            advantages[t] = gae\n",
        "            returns[t] = advantages[t] + values[t]\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "    def _cg(self, Ax, b, iters=10):\n",
        "        x = torch.zeros_like(b)\n",
        "        r = b.clone()\n",
        "        p = b.clone()\n",
        "        r_dot_r = torch.dot(r, r)\n",
        "\n",
        "        for _ in range(iters):\n",
        "            Ap = Ax(p)\n",
        "            alpha = r_dot_r / (torch.dot(p, Ap) + self.damping)\n",
        "\n",
        "            x += alpha * p\n",
        "            r -= alpha * Ap\n",
        "\n",
        "            r_dot_r_new = torch.dot(r, r)\n",
        "            beta = r_dot_r_new / r_dot_r\n",
        "            r_dot_r = r_dot_r_new\n",
        "\n",
        "            if r_dot_r < 1e-10:\n",
        "                break\n",
        "\n",
        "            p = r + beta * p\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _hessian_vector_product(self, states, old_policy, vector):\n",
        "        kl = self.policy.get_kl(states, old_policy)\n",
        "\n",
        "        grads = torch.autograd.grad(kl, self.policy.parameters(), create_graph=True)\n",
        "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "        kl_v = (flat_grad_kl * vector).sum()\n",
        "        grads = torch.autograd.grad(kl_v, self.policy.parameters())\n",
        "        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads])\n",
        "\n",
        "        return flat_grad_grad_kl\n",
        "\n",
        "    def _surrogate_loss(self, states, actions, advantages, old_log_probs):\n",
        "        log_probs = self.policy.log_prob(states, actions)\n",
        "        ratio = torch.exp(log_probs - old_log_probs)\n",
        "        return (ratio * advantages).mean()\n",
        "\n",
        "    def update(self, states, actions, rewards, masks):\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.FloatTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        masks = torch.FloatTensor(masks)\n",
        "\n",
        "        # Update value function\n",
        "        values = self.value(states)\n",
        "        advantages, returns = self._compute_advantages(rewards, values, masks)\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # Value loss\n",
        "        value_loss = -F.mse_loss(self.value(states), returns)\n",
        "        self.value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.value_optimizer.step()\n",
        "\n",
        "        # Policy gradient\n",
        "        old_policy = PolicyNetwork(states.shape[1], actions.shape[1])\n",
        "        old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        old_log_probs = old_policy.log_prob(states, actions).detach()\n",
        "\n",
        "        # Compute policy gradient\n",
        "        loss = self._surrogate_loss(states, actions, advantages, old_log_probs)\n",
        "        grads = torch.autograd.grad(loss, self.policy.parameters())\n",
        "        flat_grad = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "        # Compute search direction with conjugate gradient\n",
        "        Ax = lambda x: self._hessian_vector_product(states, old_policy, x)  # noqa: E731\n",
        "        step_dir = self._cg(Ax, flat_grad, self.cg_iters)\n",
        "\n",
        "        # Compute step size with line search\n",
        "        shs = 0.5 * (step_dir * Ax(step_dir)).sum(0, keepdim=True)\n",
        "        lm = torch.sqrt(shs / self.delta)\n",
        "        full_step = step_dir / lm\n",
        "\n",
        "        # Get current parameters\n",
        "        params = torch.cat([param.view(-1) for param in self.policy.parameters()])\n",
        "\n",
        "        # Line search\n",
        "        expected_improve = (flat_grad * full_step).sum(0, keepdim=True).item()\n",
        "\n",
        "        success = False\n",
        "        for i in range(self.backtrack_iters):\n",
        "            step_size = self.backtrack_coeff**i\n",
        "            new_params = params + step_size * full_step\n",
        "\n",
        "            # Update policy parameters\n",
        "            idx = 0\n",
        "            for param in self.policy.parameters():\n",
        "                param_size = param.numel()\n",
        "                param.data.copy_(new_params[idx : idx + param_size].view(param.size()))\n",
        "                idx += param_size\n",
        "\n",
        "            # Compute new loss\n",
        "            new_loss = self._surrogate_loss(\n",
        "                states, actions, advantages, old_log_probs\n",
        "            ).item()\n",
        "\n",
        "            # Check improvement\n",
        "            improve = new_loss - loss.item()\n",
        "            if improve > 0.1 * step_size * expected_improve:\n",
        "                success = True\n",
        "                break\n",
        "\n",
        "        if not success:\n",
        "            # Restore old parameters\n",
        "            idx = 0\n",
        "            for param in self.policy.parameters():\n",
        "                param_size = param.numel()\n",
        "                param.data.copy_(params[idx : idx + param_size].view(param.size()))\n",
        "                idx += param_size\n",
        "\n",
        "        return {\n",
        "            \"policy_loss\": -loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"kl\": self.policy.get_kl(states, old_policy).item(),\n",
        "            \"entropy\": -old_log_probs.mean().item(),\n",
        "        }\n",
        "```\n",
        "\n",
        "Entropy function for PPO\n",
        "\n",
        "```python\n",
        "def entropy(self, state):\n",
        "    _, std = self.forward(state)\n",
        "    return torch.log(std * torch.sqrt(torch.tensor(2 * np.pi * np.e))).sum(dim=-1)\n",
        "```\n",
        "\n",
        "PPO code snippet\n",
        "\n",
        "```python\n",
        "class PPO:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 64,\n",
        "        lr: float = 3e-4,\n",
        "        gamma: float = 0.99,\n",
        "        tau: float = 0.95,\n",
        "        clip_param: float = 0.2,\n",
        "        ppo_epochs: int = 10,\n",
        "        mini_batch_size: int = 64,\n",
        "        entropy_coef: float = 0.01,\n",
        "        value_coef: float = 0.5,\n",
        "    ):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.clip_param = clip_param\n",
        "        self.ppo_epochs = ppo_epochs\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.value_coef = value_coef\n",
        "\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        self.value = ValueNetwork(state_dim, hidden_dim)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {\"params\": self.policy.parameters(), \"lr\": lr},\n",
        "            {\"params\": self.value.parameters(), \"lr\": lr},\n",
        "        ])\n",
        "\n",
        "    def _compute_advantages(self, rewards, values, masks):\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        gae = 0\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value = 0\n",
        "            else:\n",
        "                next_value = values[t + 1]\n",
        "\n",
        "            delta = rewards[t] + self.gamma * next_value * masks[t] - values[t]\n",
        "            gae = delta + self.gamma * self.tau * masks[t] * gae\n",
        "\n",
        "            advantages[t] = gae\n",
        "            returns[t] = advantages[t] + values[t]\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "    def update(self, states, actions, rewards, masks):\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.FloatTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        masks = torch.FloatTensor(masks)\n",
        "\n",
        "        # Compute values and advantages\n",
        "        with torch.no_grad():\n",
        "            values = self.value(states)\n",
        "            advantages, returns = self._compute_advantages(rewards, values, masks)\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # Get old log probabilities\n",
        "        with torch.no_grad():\n",
        "            old_log_probs = self.policy.log_prob(states, actions)\n",
        "\n",
        "        # Create dataset and dataloader for mini-batch updates\n",
        "        dataset = TensorDataset(states, actions, old_log_probs, advantages, returns)\n",
        "        dataloader = DataLoader(dataset, batch_size=self.mini_batch_size, shuffle=True)\n",
        "\n",
        "        # Track metrics\n",
        "        policy_loss_epoch = 0\n",
        "        value_loss_epoch = 0\n",
        "        entropy_epoch = 0\n",
        "        kl_epoch = 0\n",
        "\n",
        "        # PPO update loop\n",
        "        for _ in range(self.ppo_epochs):\n",
        "            for (\n",
        "                batch_states,\n",
        "                batch_actions,\n",
        "                batch_old_log_probs,\n",
        "                batch_advantages,\n",
        "                batch_returns,\n",
        "            ) in dataloader:\n",
        "                # Policy loss\n",
        "                log_probs = self.policy.log_prob(batch_states, batch_actions)\n",
        "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
        "\n",
        "                # Calculate KL divergence (for logging only)\n",
        "                kl_div = (batch_old_log_probs - log_probs).mean().item()\n",
        "\n",
        "                # Clipped policy objective\n",
        "                surrogate1 = ratio * batch_advantages\n",
        "                surrogate2 = (\n",
        "                    torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param)\n",
        "                    * batch_advantages\n",
        "                )\n",
        "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value_pred = self.value(batch_states)\n",
        "                value_loss = F.mse_loss(value_pred, batch_returns)\n",
        "\n",
        "                # Entropy bonus\n",
        "                entropy = self.policy.entropy(batch_states).mean()\n",
        "\n",
        "                # Total loss\n",
        "                loss = (\n",
        "                    policy_loss\n",
        "                    + self.value_coef * value_loss\n",
        "                    - self.entropy_coef * entropy\n",
        "                )\n",
        "\n",
        "                # Update networks\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Track metrics\n",
        "                policy_loss_epoch += policy_loss.item()\n",
        "                value_loss_epoch += value_loss.item()\n",
        "                entropy_epoch += entropy.item()\n",
        "                kl_epoch += kl_div\n",
        "\n",
        "        # Average metrics over all batches\n",
        "        num_batches = len(dataloader) * self.ppo_epochs\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss_epoch / num_batches,\n",
        "            \"value_loss\": value_loss_epoch / num_batches,\n",
        "            \"entropy\": entropy_epoch / num_batches,\n",
        "            \"kl\": kl_epoch / num_batches,\n",
        "        }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and visualization\n",
        "\n",
        "Here we are present evaluation of our models with pretrained weights that we train in `PPO.ipynb` and `TRPO.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation of TPPO models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Standart reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\results folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "TRPO evaluation: 100%|██████████| 1000/1000 [03:52<00:00,  4.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results over 1000 episodes:\n",
            "Average Episode Length: 977.67\n",
            "Average Episode Reward: 9149.17\n",
            "Average Episode Energy Reward: 11498.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Video files conversion: 100%|██████████| 1/1 [00:21<00:00, 21.58s/it]\n"
          ]
        }
      ],
      "source": [
        "agent = load_model(\"./results/trpo-rewards-best.pt\", agent)\n",
        "evaluate(ENV_NAME, agent, num_episodes=1000, record_video=True, reward_type=\"rewards\")\n",
        "\n",
        "# Convert all mp4 files in the results folder to gif\n",
        "mp4_to_gif(\"./results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"600\" src=\"results\\trpo-rewards-episode-0.gif\">\n",
        "    <p align=\"center\">Fig. 2 - TRPO agent with energy based reward</p>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Energy reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\results folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "TRPO evaluation: 100%|██████████| 1000/1000 [00:52<00:00, 19.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results over 1000 episodes:\n",
            "Average Episode Length: 193.67\n",
            "Average Episode Reward: 1799.79\n",
            "Average Episode Energy Reward: 2236.88\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Video files conversion: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n"
          ]
        }
      ],
      "source": [
        "agent = load_model(\"./results/trpo-energies-best.pt\", agent)\n",
        "evaluate(ENV_NAME, agent, num_episodes=1000, record_video=True, reward_type=\"energies\")\n",
        "\n",
        "# Convert all mp4 files in the results folder to gif\n",
        "mp4_to_gif(\"./results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"600\" src=\"results\\trpo-energies-episode-0.gif\">\n",
        "    <p align=\"center\">Fig. 2 - TRPO agent with energy based reward</p>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluation code snippet**\n",
        "\n",
        "```python\n",
        "def evaluate(\n",
        "    env_name: str,\n",
        "    agent: PPO | TRPO,\n",
        "    num_episodes: int = 10,\n",
        "    record_video: bool = True,\n",
        "    reward_type: str = \"reward\",\n",
        "):\n",
        "    name = agent.__class__.__name__.lower()\n",
        "\n",
        "    # Create environment\n",
        "    if record_video:\n",
        "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "        env = gym.wrappers.RecordVideo(\n",
        "            env,\n",
        "            \"results\",\n",
        "            episode_trigger=lambda x: x < 1,\n",
        "            disable_logger=True,\n",
        "            name_prefix=f\"{name}-{reward_type}\",\n",
        "        )\n",
        "    else:\n",
        "        env = gym.make(env_name)\n",
        "\n",
        "    # Create energy reward calculator\n",
        "    energy_reward_calculator = EnergyReward()\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_energy_rewards = []\n",
        "    episode_lengths = []\n",
        "\n",
        "    # Create CSV logger\n",
        "    os.makedirs(\"./results\", exist_ok=True)\n",
        "    csv_file = open(f\"./results/{name}-eval-{reward_type}.csv\", \"w\", newline=\"\")\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\n",
        "        \"length\",\n",
        "        \"reward\",\n",
        "        \"energy\",\n",
        "    ])\n",
        "\n",
        "    for _ in tqdm.trange(num_episodes, desc=f\"{name.upper()} evaluation\"):\n",
        "        state, _ = env.reset(seed=np.random.randint(10000))\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_energy = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        while not done:\n",
        "            # Use deterministic action for evaluation\n",
        "            with torch.no_grad():\n",
        "                action = (\n",
        "                    agent.policy.get_action(\n",
        "                        torch.FloatTensor(state), deterministic=True\n",
        "                    )\n",
        "                    .detach()\n",
        "                    .numpy()\n",
        "                )\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Calculate energy-based reward\n",
        "            energy_reward = energy_reward_calculator(state)\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_energy += energy_reward\n",
        "            episode_length += 1\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        csv_writer.writerow([episode_length, episode_reward, episode_energy])\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_energy_rewards.append(episode_energy)\n",
        "        episode_lengths.append(episode_length)\n",
        "\n",
        "    data = pd.DataFrame(columns=[\"length\", \"reward\", \"energy\"])\n",
        "    data[\"length\"] = episode_lengths\n",
        "    data[\"reward\"] = episode_rewards\n",
        "    data[\"energy\"] = episode_energy_rewards\n",
        "    data.to_csv(f\"./results/{name}-eval-{reward_type}.csv\", index=False)\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "    avg_energy_reward = np.mean(episode_energy_rewards)\n",
        "    avg_length = np.mean(episode_lengths)\n",
        "\n",
        "    print(f\"Evaluation Results over {num_episodes} episodes:\")\n",
        "    print(f\"Average Episode Length: {avg_length:.2f}\")\n",
        "    print(f\"Average Episode Reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average Episode Energy Reward: {avg_energy_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return avg_reward, avg_energy_reward, avg_length\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation of PPO models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Standart reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\results folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "PPO evaluation: 100%|██████████| 1000/1000 [03:49<00:00,  4.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results over 1000 episodes:\n",
            "Average Episode Length: 994.11\n",
            "Average Episode Reward: 9303.52\n",
            "Average Episode Energy Reward: 11694.95\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Video files conversion: 100%|██████████| 1/1 [00:16<00:00, 16.58s/it]\n"
          ]
        }
      ],
      "source": [
        "agent = load_model(\"./results/ppo-rewards-best.pt\", agent)\n",
        "evaluate(ENV_NAME, agent, num_episodes=1000, record_video=True, reward_type=\"rewards\")\n",
        "\n",
        "# Convert all mp4 files in the results folder to gif\n",
        "mp4_to_gif(\"./results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"600\" src=\"results\\ppo-rewards-episode-0.gif\">\n",
        "    <p align=\"center\">Fig. 3 - PPO agent with standart reward</p>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Energy reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\mekor\\Documents\\Skoltech\\Term-7\\RL\\energy-based-ppo\\results folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "PPO evaluation: 100%|██████████| 1000/1000 [03:51<00:00,  4.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results over 1000 episodes:\n",
            "Average Episode Length: 994.11\n",
            "Average Episode Reward: 9303.23\n",
            "Average Episode Energy Reward: 11695.24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Video files conversion: 100%|██████████| 1/1 [00:17<00:00, 17.06s/it]\n"
          ]
        }
      ],
      "source": [
        "agent = load_model(\"./results/ppo-energies-best.pt\", agent)\n",
        "evaluate(ENV_NAME, agent, num_episodes=1000, record_video=True, reward_type=\"energies\")\n",
        "\n",
        "# Convert all mp4 files in the results folder to gif\n",
        "mp4_to_gif(\"./results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"600\" src=\"results\\ppo-energies-episode-0.gif\">\n",
        "    <p align=\"center\">Fig. 4 - PPO agent with energy based reward</p>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarking with Baseline Agent\n",
        "\n",
        "Our agents can be benchmarked against baseline agents (e.g., Random Agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"600\" src=\"results\\random-episode-0.gif\">\n",
        "    <p align=\"center\">Fig. 5 - Random agent</p>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
